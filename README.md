# Delhi High Court Website Scraper #

# README #

This project uses splash to first render a webpage in docker and then parse it using scrapy. This approach is well suited for JS enabled websites.

### The Working ###

* The URLs that need to be parsed are first placed in the urls.txt file
* The case type, case number, case year, petitioner, respondent, advocate, court number, next date and last date and city is parsed from the given url page
* The code will send a GET request to the sever to get 'adgallery.xml' which contains the names and pages at which the ads are present.
* BeautifulSoup is then used to parse the XML document.
* Careful inspection of the website reveals that the ad page url can be generated by a simple combination of above mentioned parameters.(this may change in future!)
* The image urls are sent to a custom scrapy Images pipeline for downloading.

### Steps To Run ###

* Docker needs to be running([Install Here](https://docs.docker.com/engine/installation/linux/docker-ce/ubuntu/#install-docker-ce))
* Pull Docker Image : `sudo docker pull scrapinghub/splash:2.3.3`
* Start Container :
`sudo docker run -d --restart=always -p 8050:8050 -p 5023:5023 scrapinghub/splash:2.3.3 --max-timeout 99999`

```
### Dependencies ###
`scrapy`
`scrapy-splash`
`requests`
`lxml`
`bs4`

 The exact dependencies can be downloaded by running `pip install -r requirements.txt`

* install requests using `pip install requests`

Run these:
`sudo apt-get install libxml2-dev libxslt1-dev imagemagick libjpeg-dev python3-dev python3-setuptools pdftk`


### Common Issues ###

* The most common issue seems to be splash. This script uses splash 2.3.3. Splashv3 was released but does not work well with the script. Always run the container with `--max-timeout` arguement set to a large value. This is the time after which splash drops the request execution, default is 60 sec.
* Scrapy is asynchronous. It generates all the requests and queues them. So there is a possibility that a request generated late in the code can be executed earlier or some other variation of this. Best possible way to overcome this is to set the `priority` arguement of scrapy in order in which you want the requests to be executed. Higher the number, higher the priority.
* Splash settings need to be set in settings.py. [See Here](https://github.com/scrapy-plugins/scrapy-splash)
* The wait times can be altered but do not decrease it way down. If you get error like image_urls not found or a variation of this then increase the wait times and run the script again. Lesser wait times might decrease program execution but if the wait period times out before the server respondes(or due to connection problems) image might not be scraped.
* For custom image pipeline [refer this video](https://www.youtube.com/watch?v=smT8kDBICAA)
* On server `sudo apt-get install build-essential libssl-dev python3-dev` as these may cause problems in scrapy installation
* If issues related to cpython exist.Use `sudo apt-get install python-lxml` `virtualenv --system-site-packages venv` `source venv/bin/activate` [refer accepted answer for this post](https://stackoverflow.com/questions/31462967/problems-installing-lxml-in-ubuntu)

### Make a virtual environment(optional) ###
* run `virtualenv -p python3 venv` inside the project directory

### Steps for Server Execution ###
* run the virtual environment by moving to the project directory `cd scraping_all_news/`
* a folder `venv` should be visible now. Run `source venv/bin/activate` to start the virtual environment
* Now `cd scraping_all_news` to get the `urls.txt` file and enter the urls to be parsed. Run the spider by `scrapy crawl scraping_all_news`. The scraped pdfs will appear in the parent folder under `Scraped Items`
